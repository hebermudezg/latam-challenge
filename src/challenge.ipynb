{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"../data/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATAM Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proyecto de Ingeniería de Datos\n",
    "\n",
    "Este proyecto aborda un desafío de ingeniería de datos que consiste en procesar un conjunto de datos de tweets relacionados con protestas agrícolas. La idea es implementar unas funciones que sean óptimas en tiempo y consumo de memoria. Además, se implementaron pruebas unitarias para garantizar la precisión de las funciones desarrolladas.\n",
    "\n",
    "## Enfoques\n",
    "\n",
    "### Enfoque Local\n",
    "Utiliza librerías básicas para procesar la información localmente.\n",
    "\n",
    "### Enfoque Distribuido\n",
    "Usa PySpark para demostrar cómo se podrían implementar soluciones similares pero para el procesamiento de grandes volúmenes de datos en entornos distribuidos, aunque los datos en este caso no son tan grandes.\n",
    "\n",
    "## Problema\n",
    "\n",
    "Se proporciona un archivo JSON de 398 MB con tweets. Se deben resolver los siguientes problemas:\n",
    "\n",
    "1. **Top 10 Fechas con Más Tweets**: Identificar las 10 fechas con más tweets y el usuario con más publicaciones en cada una.\n",
    "2. **Top 10 Emojis Más Usados**: Identificar los 10 emojis más usados y su conteo.\n",
    "3. **Top 10 Usuarios Más Mencionados**: Identificar los 10 usuarios más influyentes según el conteo de menciones (@).\n",
    "\n",
    "Cada problema se resolverá optimizando el tiempo de ejecución y el uso de memoria.\n",
    "\n",
    "## Pruebas Unitarias\n",
    "\n",
    "Se implementaron pruebas unitarias para cada una de las funciones desarrolladas, garantizando así la exactitud de los resultados. Las pruebas aseguran que las funciones manejen correctamente los casos límite y los posibles errores. A continuación se detallan las pruebas realizadas:\n",
    "\n",
    "1. **Top 10 Fechas con Más Tweets**:\n",
    "    - **Pruebas Básicas**: Verificar que las funciones `q1_time` y `q1_memory` identifiquen correctamente las fechas con más tweets y el usuario más activo en esas fechas.\n",
    "    - **Casos Límite**: Verificar el comportamiento de las funciones cuando hay tweets con fechas faltantes o formatos de fecha incorrectos, manejando estos casos con excepciones y retornando resultados consistentes.\n",
    "\n",
    "2. **Top 10 Emojis Más Usados**:\n",
    "    - **Pruebas Básicas**: Verificar que las funciones `q2_time` y `q2_memory` identifiquen correctamente los emojis más usados y su conteo.\n",
    "    - **Casos Límite**: Verificar el comportamiento de las funciones cuando hay tweets con contenido faltante, asegurando que los emojis se cuenten correctamente y que los casos con contenido faltante sean manejados sin causar errores.\n",
    "\n",
    "3. **Top 10 Usuarios Más Mencionados**:\n",
    "    - **Pruebas Básicas**: Verificar que las funciones `q3_time` y `q3_memory` identifiquen correctamente los usuarios más mencionados y su conteo.\n",
    "    - **Casos Límite**: Verificar el comportamiento de las funciones cuando hay tweets con contenido faltante o sin menciones, asegurando que los usuarios se cuenten correctamente y que los casos con contenido faltante sean manejados adecuadamente.\n",
    "\n",
    "Las pruebas unitarias fueron diseñadas para cubrir diferentes escenarios, incluyendo datos válidos y casos límite, para asegurar que las funciones desarrolladas son medianamente fiables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>tcooutlinks</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>...</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>conversationId</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>sourceLabel</th>\n",
       "      <th>media</th>\n",
       "      <th>retweetedTweet</th>\n",
       "      <th>quotedTweet</th>\n",
       "      <th>mentionedUsers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/ArjunSinghPanam/status/136...</td>\n",
       "      <td>2021-02-24 09:23:35+00:00</td>\n",
       "      <td>The world progresses while the Indian police a...</td>\n",
       "      <td>The world progresses while the Indian police a...</td>\n",
       "      <td>1364506249291784198</td>\n",
       "      <td>{'username': 'ArjunSinghPanam', 'displayname':...</td>\n",
       "      <td>[https://twitter.com/ravisinghka/status/136415...</td>\n",
       "      <td>[https://t.co/es3kn0IQAF]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506249291784198</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'url': 'https://twitter.com/RaviSinghKA/statu...</td>\n",
       "      <td>[{'username': 'narendramodi', 'displayname': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/PrdeepNain/status/13645062...</td>\n",
       "      <td>2021-02-24 09:23:32+00:00</td>\n",
       "      <td>#FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...</td>\n",
       "      <td>#FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...</td>\n",
       "      <td>1364506237451313155</td>\n",
       "      <td>{'username': 'PrdeepNain', 'displayname': 'Pra...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506237451313155</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>[{'thumbnailUrl': 'https://pbs.twimg.com/ext_t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'username': 'Kisanektamorcha', 'displayname'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/parmarmaninder/status/1364...</td>\n",
       "      <td>2021-02-24 09:23:22+00:00</td>\n",
       "      <td>ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...</td>\n",
       "      <td>ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...</td>\n",
       "      <td>1364506195453767680</td>\n",
       "      <td>{'username': 'parmarmaninder', 'displayname': ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506195453767680</td>\n",
       "      <td>pa</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://twitter.com/ArjunSinghPanam/status/136...   \n",
       "1  https://twitter.com/PrdeepNain/status/13645062...   \n",
       "2  https://twitter.com/parmarmaninder/status/1364...   \n",
       "\n",
       "                       date  \\\n",
       "0 2021-02-24 09:23:35+00:00   \n",
       "1 2021-02-24 09:23:32+00:00   \n",
       "2 2021-02-24 09:23:22+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  The world progresses while the Indian police a...   \n",
       "1  #FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...   \n",
       "2  ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...   \n",
       "\n",
       "                                     renderedContent                   id  \\\n",
       "0  The world progresses while the Indian police a...  1364506249291784198   \n",
       "1  #FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...  1364506237451313155   \n",
       "2  ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...  1364506195453767680   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'username': 'ArjunSinghPanam', 'displayname':...   \n",
       "1  {'username': 'PrdeepNain', 'displayname': 'Pra...   \n",
       "2  {'username': 'parmarmaninder', 'displayname': ...   \n",
       "\n",
       "                                            outlinks  \\\n",
       "0  [https://twitter.com/ravisinghka/status/136415...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "\n",
       "                 tcooutlinks  replyCount  retweetCount  ...  quoteCount  \\\n",
       "0  [https://t.co/es3kn0IQAF]           0             0  ...           0   \n",
       "1                         []           0             0  ...           0   \n",
       "2                         []           0             0  ...           0   \n",
       "\n",
       "        conversationId  lang  \\\n",
       "0  1364506249291784198    en   \n",
       "1  1364506237451313155    en   \n",
       "2  1364506195453767680    pa   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                             sourceUrl          sourceLabel  \\\n",
       "0   http://twitter.com/download/iphone   Twitter for iPhone   \n",
       "1  http://twitter.com/download/android  Twitter for Android   \n",
       "2  http://twitter.com/download/android  Twitter for Android   \n",
       "\n",
       "                                               media retweetedTweet  \\\n",
       "0                                               None            NaN   \n",
       "1  [{'thumbnailUrl': 'https://pbs.twimg.com/ext_t...            NaN   \n",
       "2                                               None            NaN   \n",
       "\n",
       "                                         quotedTweet  \\\n",
       "0  {'url': 'https://twitter.com/RaviSinghKA/statu...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "\n",
       "                                      mentionedUsers  \n",
       "0  [{'username': 'narendramodi', 'displayname': '...  \n",
       "1  [{'username': 'Kisanektamorcha', 'displayname'...  \n",
       "2                                               None  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción de las Columnas del DataFrame\n",
    "\n",
    "1. **url**: La URL del tweet.\n",
    "2. **date**: La fecha y hora en que se creó el tweet, en formato UTC.\n",
    "3. **content**: El texto del tweet.\n",
    "4. **renderedContent**: El contenido renderizado del tweet (puede ser igual a `content`).\n",
    "5. **id**: El identificador único del tweet.\n",
    "6. **user**: Informacion sobre usuario que publicó el tweet.\n",
    "7. **outlinks**: Enlaces externos incluidos en el tweet.\n",
    "8. **tcooutlinks**: Enlaces acortados de Twitter incluidos en el tweet.\n",
    "9. **replyCount**: Número de respuestas al tweet.\n",
    "10. **retweetCount**: Número de retweets del tweet.\n",
    "11. **likeCount**: Número de \"me gusta\" que recibió el tweet.\n",
    "12. **quoteCount**: Número de veces que el tweet fue citado.\n",
    "13. **conversationId**: Identificador de la conversación a la que pertenece el tweet.\n",
    "14. **lang**: El idioma del tweet, en formato de código BCP 47.\n",
    "15. **source**: La fuente desde donde se publicó el tweet (por ejemplo, \"Twitter Web Client\").\n",
    "16. **sourceUrl**: URL de la fuente desde donde se publicó el tweet.\n",
    "17. **sourceLabel**: Etiqueta de la fuente desde donde se publicó el tweet.\n",
    "18. **media**: Información sobre los medios incluidos en el tweet (imágenes, videos, etc.).\n",
    "19. **retweetedTweet**: Información sobre el tweet original si este es un retweet.\n",
    "20. **quotedTweet**: Información sobre el tweet citado.\n",
    "21. **mentionedUsers**: Usuarios mencionados en el tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque Local\n",
    "---\n",
    "\n",
    "Las funciones `qx_time` y `qx_memory` fueron optimizadas para tiempo y memoria respectivamente. Para `q_time`, se utilizó `defaultdict` para contar tweets por fecha y usuario, procesando el archivo JSON línea por línea con `open` y `json.loads`, lo que permitió una rápida conversión de cada línea en un diccionario de Python. Los resultados se calcularon ordenando y agregando eficientemente los datos. Para `q_memory`, se implementaron generadores (`tweet_generator`) que leen el archivo línea por línea, reduciendo significativamente el uso de memoria. Al igual que en `q_time`, `defaultdict` se utilizó para llevar un conteo incremental de tweets, emojis o menciones, evitando cargar todo el archivo en memoria y optimizando así el rendimiento en términos de consumo de recursos. Aunque la librería pandas podría ser eficiente en algunos casos, se optó por estos enfoques para manejar eficientemente grandes volúmenes de datos.\n",
    "\n",
    "### Las Top 10 Fechas con Más Tweets y el Usuario Más Activo en Cada Fecha\n",
    "\n",
    "| Función         | Tiempo de Ejecución (s) | Uso de Memoria (MiB) |\n",
    "|-----------------|--------------------------|----------------------|\n",
    "| `q1_time`       | 2.96                     | 1.69                 |\n",
    "| `q1_memory`     | 2.83                     | 0.05                 |\n",
    "\n",
    "### Los Top 10 Emojis Más Usados con su Respectivo Conteo\n",
    "\n",
    "| Función         | Tiempo de Ejecución (s) | Uso de Memoria (MiB) |\n",
    "|-----------------|--------------------------|----------------------|\n",
    "| `q2_time`       | 3.20                     | 1.23                 |\n",
    "| `q2_memory`     | 3.24                     | 0.14                 |\n",
    "\n",
    "### Top 10 Usuarios Más Mencionados\n",
    "\n",
    "| Función         | Tiempo de Ejecución (s) | Uso de Memoria (MiB) |\n",
    "|-----------------|--------------------------|----------------------|\n",
    "| `q3_time`       | 2.65                     | 61.86                |\n",
    "| `q3_memory`     | 2.64                     | 0.27                 |\n",
    "\n",
    "\n",
    "\n",
    "NOTA: Se se ejecuta nuevamente las celdas puede cambiar brevemente el resultado. Se puede apreciar que qx_memory se ha logrado optimizar existosamente para consumo de momoria, en los qx_time los resultaods no son significativamente diferetes en terminos de tiempo que las funciones qx_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importando librerías necesarias\n",
    "import pandas as pd\n",
    "import time\n",
    "from memory_profiler import profile\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Las Top 10 Fechas con Más Tweets y el Usuario Más Activo en Cada Fecha</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Procesamiento completado exitosamente\n",
      "[INFO] Tiempo de ejecución de q1_time: 2.96 segundos\n",
      "[INFO] Uso de memoria de q1_time: 1.69 MiB\n",
      "[INFO] Procesamiento completado exitosamente\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de las\n",
    "    10 fechas con más tweets y el nombre del usuario que más publicó en cada una de esas fechas,\n",
    "    optimizando para el uso de memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[datetime.date, str]]: Una lista de tuplas donde cada tupla contiene una fecha (datetime.date)\n",
    "    y el nombre del usuario que más publicó en esa fecha.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Estructura para contar los tweets por fecha y por usuario\n",
    "        date_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        # Procesar el archivo línea por línea para minimizar el uso de memoria\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                tweet = json.loads(line) # Convertir a diccionario de Python\n",
    "                try:\n",
    "                    # Extraer y validar la fecha del tweet\n",
    "                    date_str = tweet.get('date', None)\n",
    "                    if date_str:\n",
    "                        date = datetime.strptime(date_str[:10], '%Y-%m-%d').date() #funcion problematica\n",
    "                    else:\n",
    "                        continue  # Saltar si no hay fecha\n",
    "\n",
    "                    # Extraer y validar el usuario del tweet\n",
    "                    user = tweet['user']['username'] if tweet['user'] and 'username' in tweet['user'] else None\n",
    "                    if user:\n",
    "                        date_counts[date][user] += 1  # Incrementar el conteo para la fecha y usuario correspondiente\n",
    "\n",
    "                except (KeyError, ValueError) as e:\n",
    "                    print(f\"[WARNING] Error procesando el tweet: {e}\")\n",
    "                    continue  # Saltar en caso de error de clave o valor\n",
    "\n",
    "        # Obtener las 10 fechas con más tweets\n",
    "        top_dates = sorted(date_counts.items(), key=lambda x: sum(x[1].values()), reverse=True)[:10] #x[1] conteos por usuario\n",
    "\n",
    "        # Crear una lista de tuplas con las fechas y el nombre de usuario más activo en cada fecha\n",
    "        result = [(date, max(users.items(), key=lambda x: x[1])[0]) for date, users in top_dates] #sum(users.values())\n",
    "\n",
    "        print(f\"[INFO] Procesamiento completado exitosamente\")\n",
    "        return result\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] El archivo {file_path} no existe.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_memory = memory_usage((q1_time, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"[INFO] Tiempo de ejecución de q1_time: {end_time - start_time:.2f} segundos\")\n",
    "print(f\"[INFO] Uso de memoria de q1_time: {max(mem_usage_memory) - min(mem_usage_memory):.2f} MiB\")\n",
    "print(q1_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Procesamiento completado exitosamente\n",
      "[INFO] Tiempo de ejecución de q1_memory: 2.83 segundos\n",
      "[INFO] Uso de memoria de q1_memory: 0.05 MiB\n",
      "[INFO] Procesamiento completado exitosamente\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de las\n",
    "    10 fechas con más tweets y el nombre del usuario que más publicó en cada una de esas fechas,\n",
    "    optimizando para el uso de memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[datetime.date, str]]: Una lista de tuplas donde cada tupla contiene una fecha (datetime.date)\n",
    "    y el nombre del usuario que más publicó en esa fecha.\n",
    "    \"\"\"\n",
    "    # Estructura para contar los tweets por fecha y por usuario\n",
    "    date_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Función generadora para procesar el archivo línea por línea\n",
    "    def tweet_generator(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    yield json.loads(line)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[ERROR] El archivo {file_path} no existe.\")\n",
    "            raise\n",
    "\n",
    "    # Usar el generador para procesar cada tweet\n",
    "    for tweet in tweet_generator(file_path):\n",
    "        try:\n",
    "            # Extraer y validar la fecha del tweet\n",
    "            date_str = tweet.get('date', None)\n",
    "            if date_str:\n",
    "                date = datetime.strptime(date_str[:10], '%Y-%m-%d').date()\n",
    "            else:\n",
    "                continue  # Saltar si no hay fecha\n",
    "\n",
    "            # Extraer y validar el usuario del tweet\n",
    "            user = tweet['user']['username'] if tweet['user'] and 'username' in tweet['user'] else None\n",
    "            if user:\n",
    "                date_counts[date][user] += 1  # Incrementar el conteo para la fecha y usuario correspondiente\n",
    "\n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(f\"[WARNING] Error procesando el tweet: {e}\")\n",
    "            continue  # Saltar en caso de error de clave o valor\n",
    "\n",
    "    # Obtener las 10 fechas con más tweets\n",
    "    top_dates = sorted(date_counts.items(), key=lambda x: sum(x[1].values()), reverse=True)[:10]\n",
    "\n",
    "    # Crear una lista de tuplas con las fechas y el nombre de usuario más activo en cada fecha\n",
    "    result = [(date, max(users.items(), key=lambda x: x[1])[0]) for date, users in top_dates]\n",
    "\n",
    "    print(f\"[INFO] Procesamiento completado exitosamente\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_memory = memory_usage((q1_memory, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"[INFO] Tiempo de ejecución de q1_memory: {end_time - start_time:.2f} segundos\")\n",
    "print(f\"[INFO] Uso de memoria de q1_memory: {max(mem_usage_memory) - min(mem_usage_memory):.2f} MiB\")\n",
    "print(q1_memory(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Los Top 10 Emojis Más Usados con su Respectivo Conteo</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Iniciando procesamiento del archivo: ../data/farmers-protest-tweets-2021-2-4.json\n",
      "[INFO] Procesamiento completado exitosamente\n",
      "[INFO] Tiempo de ejecución de q2_time: 3.20 segundos\n",
      "[INFO] Uso de memoria de q2_time: 1.23 MiB\n",
      "[INFO] Iniciando procesamiento del archivo: ../data/farmers-protest-tweets-2021-2-4.json\n",
      "[INFO] Procesamiento completado exitosamente\n",
      "[('🙏', 7286), ('😂', 3072), ('🚜', 2972), ('✊', 2411), ('🌾', 2363), ('🏻', 2080), ('❤', 1779), ('🤣', 1668), ('🏽', 1218), ('👇', 1108)]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import emoji\n",
    "\n",
    "def extract_emojis_from_text(text: str) -> List[str]:\n",
    "    return [c for c in text if c in emoji.EMOJI_DATA]\n",
    "\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de los\n",
    "    10 emojis más usados con su respectivo conteo, optimizando para el uso de memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[str, int]]: Una lista de tuplas donde cada tupla contiene un emoji (str) y su conteo (int).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[INFO] Iniciando procesamiento del archivo: {file_path}\")\n",
    "\n",
    "        # Estructura para contar los emojis\n",
    "        emoji_counts = defaultdict(int)\n",
    "\n",
    "        # Procesar el archivo línea por línea para minimizar el uso de memoria\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    tweet = json.loads(line)  # Convertir la línea en un diccionario de Python\n",
    "                    content = tweet.get('content', '')  # Obtener el contenido del tweet, por defecto vacío si no existe\n",
    "                    if content is not None:\n",
    "                        # Extraer emojis del contenido del tweet\n",
    "                        emojis_in_tweet = extract_emojis_from_text(content)\n",
    "                        # Contar la frecuencia de cada emoji\n",
    "                        for em in emojis_in_tweet:\n",
    "                            emoji_counts[em] += 1\n",
    "                except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "                    # Captura y registra cualquier error que ocurra durante el procesamiento del tweet\n",
    "                    print(f\"[WARNING] Error procesando el tweet: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Obtener los 10 emojis más usados\n",
    "        top_emojis = sorted(emoji_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        print(f\"[INFO] Procesamiento completado exitosamente\")\n",
    "        return top_emojis\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # Manejar el caso en que el archivo no exista\n",
    "        print(f\"[ERROR] El archivo {file_path} no existe.\")\n",
    "        return []\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_memory = memory_usage((q2_time, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"[INFO] Tiempo de ejecución de q2_time: {end_time - start_time:.2f} segundos\")\n",
    "print(f\"[INFO] Uso de memoria de q2_time: {max(mem_usage_memory) - min(mem_usage_memory):.2f} MiB\")\n",
    "\n",
    "# Llamar a la función una vez más para obtener el resultado\n",
    "result = q2_time(file_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Procesamiento completado exitosamente\n",
      "[INFO] Tiempo de ejecución de q2_memory: 3.24 segundos\n",
      "[INFO] Uso de memoria de q2_memory: 0.14 MiB\n",
      "[INFO] Procesamiento completado exitosamente\n",
      "[('🙏', 7286), ('😂', 3072), ('🚜', 2972), ('✊', 2411), ('🌾', 2363), ('🏻', 2080), ('❤', 1779), ('🤣', 1668), ('🏽', 1218), ('👇', 1108)]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import emoji\n",
    "\n",
    "def extract_emojis_from_text(text):\n",
    "    return [c for c in text if c in emoji.EMOJI_DATA]\n",
    "\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de los\n",
    "    10 emojis más usados con su respectivo conteo, optimizando para el uso de memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[str, int]]: Una lista de tuplas donde cada tupla contiene un emoji (str) y su conteo (int).\n",
    "    \"\"\"\n",
    "\n",
    "    # Estructura para contar los emojis\n",
    "    emoji_counts = defaultdict(int)\n",
    "\n",
    "    # Función generadora para procesar el archivo línea por línea\n",
    "    def tweet_generator(file_path): #iterador\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    yield json.loads(line)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[ERROR] El archivo {file_path} no existe.\")\n",
    "            raise\n",
    "    # Usar el generador para procesar cada tweet\n",
    "    for tweet in tweet_generator(file_path):\n",
    "        try:\n",
    "            content = tweet.get('content', '')\n",
    "            if content is not None:\n",
    "                emojis_in_tweet = extract_emojis_from_text(content)\n",
    "                for em in emojis_in_tweet:\n",
    "                    emoji_counts[em] += 1\n",
    "        except KeyError as e:\n",
    "            print(f\"[WARNING] Error procesando el tweet: {e}\")\n",
    "\n",
    "    # Obtener los 10 emojis más usados\n",
    "    top_emojis = sorted(emoji_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    print(f\"[INFO] Procesamiento completado exitosamente\")\n",
    "    return top_emojis\n",
    "\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_memory = memory_usage((q2_memory, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"[INFO] Tiempo de ejecución de q2_memory: {end_time - start_time:.2f} segundos\")\n",
    "print(f\"[INFO] Uso de memoria de q2_memory: {max(mem_usage_memory) - min(mem_usage_memory):.2f} MiB\")\n",
    "\n",
    "# Llamar a la función una vez más para obtener el resultado\n",
    "result = q2_memory(file_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Top 10 Histórico de Usuarios Más Influyentes en Función del Conteo de las Menciones (@)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Iniciando procesamiento del archivo: ../data/farmers-protest-tweets-2021-2-4.json\n",
      "[INFO] Procesamiento completado exitosamente\n",
      "[INFO] Tiempo de ejecución de q3_time: 2.65 segundos\n",
      "[INFO] Uso de memoria de q3_time: 61.86 MiB\n",
      "[INFO] Iniciando procesamiento del archivo: ../data/farmers-protest-tweets-2021-2-4.json\n",
      "[INFO] Procesamiento completado exitosamente\n",
      "[('@narendramodi', 2261), ('@Kisanektamorcha', 1836), ('@RakeshTikaitBKU', 1639), ('@PMOIndia', 1422), ('@RahulGandhi', 1125), ('@GretaThunberg', 1046), ('@RaviSinghKA', 1015), ('@rihanna', 972), ('@UNHumanRights', 962), ('@meenaharris', 925)]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "def extract_mentions(s: str) -> List[str]:\n",
    "    return re.findall(r'@\\w+', s)\n",
    "\n",
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de los\n",
    "    10 usuarios más mencionados con su respectivo conteo, optimizando para el uso de memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[str, int]]: Una lista de tuplas donde cada tupla contiene un usuario (str) y su conteo de menciones (int).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[INFO] Iniciando procesamiento del archivo: {file_path}\")\n",
    "\n",
    "        # Estructura para contar las menciones\n",
    "        mention_counts = defaultdict(int)\n",
    "\n",
    "        # Procesar el archivo línea por línea para minimizar el uso de memoria\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    tweet = json.loads(line)  # Convertir la línea en un diccionario de Python\n",
    "                    content = tweet.get('content', '')  # Obtener el contenido del tweet, por defecto vacío si no existe\n",
    "                    if content is not None:\n",
    "                        # Extraer menciones del contenido del tweet\n",
    "                        mentions_in_tweet = extract_mentions(content)\n",
    "                        # Contar la frecuencia de cada mención\n",
    "                        for mention in mentions_in_tweet:\n",
    "                            mention_counts[mention] += 1\n",
    "                except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "                    # Capturar y registrar cualquier error que ocurra durante el procesamiento del tweet\n",
    "                    print(f\"[WARNING] Error procesando el tweet: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Obtener los 10 usuarios más mencionados\n",
    "        top_mentions = sorted(mention_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        print(f\"[INFO] Procesamiento completado exitosamente\")\n",
    "        return top_mentions\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # Manejar el caso en que el archivo no exista\n",
    "        print(f\"[ERROR] El archivo {file_path} no existe.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_memory = memory_usage((q3_time, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"[INFO] Tiempo de ejecución de q3_time: {end_time - start_time:.2f} segundos\")\n",
    "print(f\"[INFO] Uso de memoria de q3_time: {max(mem_usage_memory) - min(mem_usage_memory):.2f} MiB\")\n",
    "\n",
    "# Llamar a la función una vez más para obtener el resultado\n",
    "result = q3_time(file_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Procesamiento completado exitosamente\n",
      "[INFO] Tiempo de ejecución de q3_memory: 2.64 segundos\n",
      "[INFO] Uso de memoria de q3_memory: 0.27 MiB\n",
      "[INFO] Procesamiento completado exitosamente\n",
      "[('@narendramodi', 2261), ('@Kisanektamorcha', 1836), ('@RakeshTikaitBKU', 1639), ('@PMOIndia', 1422), ('@RahulGandhi', 1125), ('@GretaThunberg', 1046), ('@RaviSinghKA', 1015), ('@rihanna', 972), ('@UNHumanRights', 962), ('@meenaharris', 925)]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "def extract_mentions(s):\n",
    "    return re.findall(r'@\\w+', s)\n",
    "\n",
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de los\n",
    "    10 usuarios más mencionados con su respectivo conteo, optimizando para el uso de memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[str, int]]: Una lista de tuplas donde cada tupla contiene un usuario (str) y su conteo de menciones (int).\n",
    "    \"\"\"\n",
    "    # Estructura para contar las menciones\n",
    "    mention_counts = defaultdict(int)\n",
    "\n",
    "    # Función generadora para procesar el archivo línea por línea\n",
    "    def tweet_generator(file_path): #iterador\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    yield json.loads(line)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[ERROR] El archivo {file_path} no existe.\")\n",
    "            raise\n",
    "\n",
    "    # Usar el generador para procesar cada tweet\n",
    "    for tweet in tweet_generator(file_path):\n",
    "        try:\n",
    "            content = tweet.get('content', '')\n",
    "            if content is not None:\n",
    "                mentions_in_tweet = extract_mentions(content)\n",
    "                for mention in mentions_in_tweet:\n",
    "                    mention_counts[mention] += 1\n",
    "        except KeyError as e:\n",
    "            print(f\"[WARNING] Error procesando el tweet: {e}\")\n",
    "\n",
    "    # Obtener los 10 usuarios más mencionados\n",
    "    top_mentions = sorted(mention_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    print(f\"[INFO] Procesamiento completado exitosamente\")\n",
    "    return top_mentions\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_memory = memory_usage((q3_memory, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"[INFO] Tiempo de ejecución de q3_memory: {end_time - start_time:.2f} segundos\")\n",
    "print(f\"[INFO] Uso de memoria de q3_memory: {max(mem_usage_memory) - min(mem_usage_memory):.2f} MiB\")\n",
    "\n",
    "# Llamar a la función una vez más para obtener el resultado\n",
    "result = q3_memory(file_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque Distribuido en la Nube\n",
    "---\n",
    "\n",
    "Las funciones `qx_time_spark` se crearon utilizando Spark, una tecnología de computación distribuida. Este enfoque es ideal para procesar grandes volúmenes de datos y aprovecha las capacidades de paralelización y escalabilidad de Spark que se va a ver reflejado en tiempos cortos de ejecución. Sin embargo, para conjuntos de datos pequeños, como los 398 MB de este ejercicio, las ventajas pueden no ser tan evidentes debido a la sobrecarga de configurar y gestionar clústeres.\n",
    "\n",
    "### Las Top 10 Fechas con Más Tweets y el Usuario Más Activo en Cada Fecha\n",
    "\n",
    "| Función             | Tiempo de Ejecución (s) | Uso de Memoria (MiB) |\n",
    "|---------------------|--------------------------|----------------------|\n",
    "| `q1_time_spark`     | 10.32                    | 9.09                 |\n",
    "\n",
    "### Los Top 10 Emojis Más Usados con su Respectivo Conteo\n",
    "\n",
    "| Función             | Tiempo de Ejecución (s) | Uso de Memoria (MiB) |\n",
    "|---------------------|--------------------------|----------------------|\n",
    "| `q2_time_spark`     | 2.40                     | 8.94                 |\n",
    "\n",
    "### Top 10 Usuarios Más Mencionados\n",
    "\n",
    "| Función             | Tiempo de Ejecución (s) | Uso de Memoria (MiB) |\n",
    "|---------------------|--------------------------|----------------------|\n",
    "| `q3_time_spark`     | 1.44                     | 0.13                 |\n",
    "\n",
    "\n",
    "\n",
    "## Ventajas de Usar un Enfoque Distribuido\n",
    "NOTA: Si se ejecutan nuevamente las celdas, los resultados pueden variar ligeramente. El consumo de momorya puede no ser una buena metrica en este caso por la dificulta del trackeo del consumo de momoria en entornos distribuidos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Las Top 10 Fechas con Más Tweets y el Usuario Más Activo en Cada Fecha</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/25 14:15:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de q1_time_spark: 10.31771206855774 segundos\n",
      "Uso de memoria de q1_time_spark: 9.09375 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/25 14:16:12 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, to_date,split, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import datetime\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Twitter Mentions Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "def q1_time_spark(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de las\n",
    "    10 fechas con más tweets y el nombre del usuario que más publicó en cada una de esas fechas.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[datetime.date, str]]: Una lista de tuplas donde cada tupla contiene una fecha (datetime.date)\n",
    "    y el nombre del usuario que más publicó en esa fecha.\n",
    "    \"\"\"\n",
    "    # Cargar el archivo JSON en un DataFrame de Spark\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # Convertir la columna 'date' a date\n",
    "    df = df.withColumn('date', to_date(col('date')))\n",
    "    \n",
    "    # Obtener las 10 fechas con más tweets\n",
    "    top_dates = df.groupBy('date').count().orderBy('count', ascending=False).limit(10).collect()\n",
    "    \n",
    "    # Crear una lista de tuplas con las fechas y el nombre de usuario más activo en cada fecha\n",
    "    result = []\n",
    "    for row in top_dates:\n",
    "        date = row['date']\n",
    "        top_user = df.filter(col('date') == date).groupBy('user.username').count().orderBy('count', ascending=False).first()['username']\n",
    "        result.append((date, top_user))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_time = memory_usage((q1_time_spark, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q1_time_spark: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q1_time_spark: {max(mem_usage_time) - min(mem_usage_time)} MiB\")\n",
    "print(q1_time_spark(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Los Top 10 Emojis Más Usados con su Respectivo Conteo</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de q2_time_spark: 2.4027657508850098 segundos\n",
      "Uso de memoria de q2_time_spark: 8.9375 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/25 14:16:19 WARN SimpleFunctionRegistry: The function extract_emojis_udf replaced a previously registered function.\n",
      "[Stage 73:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 100846), ('🙏', 7286), ('😂', 3072), ('🚜', 2972), ('✊', 2411), ('🌾', 2363), ('🏻', 2080), ('❤', 1779), ('🤣', 1668), ('🏽', 1218)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "def extract_emojis_from_text(text):\n",
    "    return ''.join([c for c in text if c in emoji.EMOJI_DATA])\n",
    "\n",
    "def q2_time_spark(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de los\n",
    "    10 emojis más usados con su respectivo conteo.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[str, int]]: Una lista de tuplas donde cada tupla contiene un emoji (str) y su conteo (int).\n",
    "    \"\"\"\n",
    "    # Cargar el archivo JSON en un DataFrame de Spark\n",
    "    df = spark.read.json(file_path)\n",
    "    \n",
    "    # Extraer todos los emojis de los tweets\n",
    "    extract_emojis_udf = spark.udf.register(\"extract_emojis_udf\", extract_emojis_from_text)\n",
    "    df = df.withColumn('emojis', extract_emojis_udf(col('content')))\n",
    "    \n",
    "    # Explode la columna de emojis\n",
    "    df = df.withColumn('emoji', explode(split(col('emojis'), '')))\n",
    "    \n",
    "    # Contar la frecuencia de cada emoji\n",
    "    emoji_counts = df.groupBy('emoji').count().orderBy('count', ascending=False).limit(10)\n",
    "    \n",
    "    # Crear una lista de tuplas con los emojis y su conteo\n",
    "    result = [(row['emoji'], row['count']) for row in emoji_counts.collect()]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_time = memory_usage((q2_time_spark, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q2_time_spark: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q2_time_spark: {max(mem_usage_time) - min(mem_usage_time)} MiB\")\n",
    "print(q2_time_spark(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Top 10 Histórico de Usuarios Más Influyentes en Función del Conteo de las Menciones (@)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de q3_time_spark: 1.4450087547302246 segundos\n",
      "Uso de memoria de q3_time_spark: 0.125 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@narendramodi', 2261), ('@Kisanektamorcha', 1836), ('@RakeshTikaitBKU', 1639), ('@PMOIndia', 1422), ('@RahulGandhi', 1125), ('@GretaThunberg', 1046), ('@RaviSinghKA', 1015), ('@rihanna', 972), ('@UNHumanRights', 962), ('@meenaharris', 925)]\n"
     ]
    }
   ],
   "source": [
    "# función extract_mentions\n",
    "import re\n",
    "def extract_mentions(s):\n",
    "    return re.findall(r'@\\w+', s)\n",
    "\n",
    "# extract_mentions como UDF\n",
    "extract_mentions_udf = udf(extract_mentions, ArrayType(StringType()))\n",
    "\n",
    "def q3_time_spark(file_path: str) -> List[Tuple[str, int]]:\n",
    "    df = spark.read.json(file_path)\n",
    "    df = df.withColumn('mentions', explode(extract_mentions_udf(col('content'))))\n",
    "    mention_counts = df.groupBy('mentions').count().orderBy('count', ascending=False).limit(10)\n",
    "    result = [(row['mentions'], row['count']) for row in mention_counts.collect()]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_time = memory_usage((q3_time_spark, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q3_time_spark: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q3_time_spark: {max(mem_usage_time) - min(mem_usage_time)} MiB\")\n",
    "print(q3_time_spark(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de Usar un Enfoque Distribuido\n",
    "\n",
    "El enfoque distribuido es ideal para procesar grandes volúmenes de datos. Plataformas como Databricks, AWS Glue, Azure Synapse Analytics y Google BigQuery facilitan la gestión de clústeres de computación distribuida.\n",
    "\n",
    "1. **Escalabilidad**: Permite añadir nodos al clúster para manejar más datos, asegurando un rendimiento consistente.\n",
    "2. **Eficiencia de Procesamiento**: Tecnologías como Apache Spark procesan datos en paralelo, reduciendo el tiempo de ejecución. (puro hardware)\n",
    "3. **Tolerancia a Fallos**: Los sistemas distribuidos reasignan tareas si un nodo falla, garantizando continuidad en el procesamiento.\n",
    "4. **Flexibilidad y Facilidad de Uso**: Proveen entornos integrados con diversas integraciones y herramientas.\n",
    "\n",
    "Sin embargo, para conjuntos de datos pequeños, como los 398 MB de este ejercicio, las ventajas pueden no ser evidentes. La sobrecarga de configurar y gestionar clústeres puede superar los beneficios del procesamiento paralelo.\n",
    "\n",
    "### Consideraciones de Escalabilidad\n",
    "\n",
    "Para conjuntos de datos más grandes, usar Spark en un clúster distribuido es eficiente y escalable. A medida que el tamaño de los datos crece, el enfoque distribuido maneja la carga adicional sin degradar el rendimiento, algo difícil de lograr con un enfoque local.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
