{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"../data/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATAM Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proyecto se centra en la resolución de un desafío de ingeniería de datos que implica el procesamiento de un conjunto de datos de tweets relacionados con protestas agrícolas, según el nombre del archivo proporcionado. Se abordarán dos formas para dar solución a este reto, optimizando tanto el tiempo de ejecución como el uso de memoria. \n",
    "\n",
    "## Enfoques\n",
    "\n",
    "### Enfoque Local\n",
    "\n",
    "El enfoque local se implementará utilizando pandas, json, entre otras librerías. Este enfoque es adecuado para el procesamiento de datos en un entorno de desarrollo local o en nube pero con pocos datos, es fácil de implementar y depurar. Sin embargo, puede no ser adecuado para conjuntos de datos muy grandes debido a limitaciones de memoria y rendimiento.\n",
    "\n",
    "\n",
    "### Enfoque Distribuido\n",
    "\n",
    "El enfoque distribuido se implementará utilizando PySpark, que es un marco de computación distribuida que permite el procesamiento de grandes volúmenes de datos de manera eficiente. Aunque en este contexto los datos no son tan grandes y no se ejecutó en un clúster, la implementación de las funciones con PySpark se propone para demostrar cómo se podría escalar el procesamiento en entornos distribuidos. Tecnologías como Databricks, AWS Glue, Google Cloud Dataproc / BigQuery y Azure HDInsight permiten ejecutar PySpark en un clúster, ofreciendo ventajas significativas para grandes conjuntos de datos. Sin embargo, dado el tamaño relativamente pequeño de los datos en este caso, no se observarían muchas ventajas asi se ejecute en un clúster.\n",
    "\n",
    "\n",
    "\n",
    "## Descripción del Problema\n",
    "\n",
    "Fue proporcionado en el reto un archivo JSON que contiene aproximadamente 398 MB de datos de tweets. Se pide resolver los siguientes problemas:\n",
    "\n",
    "1. **Top 10 Fechas con Más Tweets**: Identificar las 10 fechas con más tweets y mencionar el usuario que más publicaciones tiene por cada una de esas fechas.\n",
    "2. **Top 10 Emojis Más Usados**: Identificar los 10 emojis más usados con su respectivo conteo.\n",
    "3. **Top 10 Usuarios Más Mencionados**: Identificar los 10 usuarios más influyentes en función del conteo de menciones (@) que registra cada uno de ellos.\n",
    "\n",
    "Cada problema se resolverá utilizando dos enfoques:\n",
    "- **Optimización para el Tiempo de Ejecución**\n",
    "- **Optimización para el Uso de Memoria**\n",
    "\n",
    "### Consideraciones sobre Tiempo y Consumo de Memoria\n",
    "\n",
    "Es importante destacar que tanto el tiempo de ejecución como el consumo de memoria dependen significativamente del hardware utilizado, así como de la metodología implementada. El rendimiento puede variar considerablemente entre diferentes configuraciones de hardware y enfoques de procesamiento de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque Local *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importando librerías necesarias\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import datetime\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from memory_profiler import profile\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "\n",
    "# Configuración de pandas para mostrar todas las columnas\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>tcooutlinks</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>conversationId</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>sourceLabel</th>\n",
       "      <th>media</th>\n",
       "      <th>retweetedTweet</th>\n",
       "      <th>quotedTweet</th>\n",
       "      <th>mentionedUsers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/ArjunSinghPanam/status/136...</td>\n",
       "      <td>2021-02-24 09:23:35+00:00</td>\n",
       "      <td>The world progresses while the Indian police a...</td>\n",
       "      <td>The world progresses while the Indian police a...</td>\n",
       "      <td>1364506249291784198</td>\n",
       "      <td>{'username': 'ArjunSinghPanam', 'displayname':...</td>\n",
       "      <td>[https://twitter.com/ravisinghka/status/136415...</td>\n",
       "      <td>[https://t.co/es3kn0IQAF]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506249291784198</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'url': 'https://twitter.com/RaviSinghKA/statu...</td>\n",
       "      <td>[{'username': 'narendramodi', 'displayname': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/PrdeepNain/status/13645062...</td>\n",
       "      <td>2021-02-24 09:23:32+00:00</td>\n",
       "      <td>#FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...</td>\n",
       "      <td>#FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...</td>\n",
       "      <td>1364506237451313155</td>\n",
       "      <td>{'username': 'PrdeepNain', 'displayname': 'Pra...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506237451313155</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>[{'thumbnailUrl': 'https://pbs.twimg.com/ext_t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'username': 'Kisanektamorcha', 'displayname'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/parmarmaninder/status/1364...</td>\n",
       "      <td>2021-02-24 09:23:22+00:00</td>\n",
       "      <td>ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...</td>\n",
       "      <td>ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...</td>\n",
       "      <td>1364506195453767680</td>\n",
       "      <td>{'username': 'parmarmaninder', 'displayname': ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506195453767680</td>\n",
       "      <td>pa</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://twitter.com/anmoldhaliwal/status/13645...</td>\n",
       "      <td>2021-02-24 09:23:16+00:00</td>\n",
       "      <td>@ReallySwara @rohini_sgh watch full video here...</td>\n",
       "      <td>@ReallySwara @rohini_sgh watch full video here...</td>\n",
       "      <td>1364506167226032128</td>\n",
       "      <td>{'username': 'anmoldhaliwal', 'displayname': '...</td>\n",
       "      <td>[https://youtu.be/-bUKumwq-J8]</td>\n",
       "      <td>[https://t.co/wBPNdJdB0n]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1364350947099484160</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>https://mobile.twitter.com</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>[{'thumbnailUrl': 'https://pbs.twimg.com/ext_t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'username': 'ReallySwara', 'displayname': 'S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://twitter.com/KotiaPreet/status/13645061...</td>\n",
       "      <td>2021-02-24 09:23:10+00:00</td>\n",
       "      <td>#KisanEktaMorcha #FarmersProtest #NoFarmersNoF...</td>\n",
       "      <td>#KisanEktaMorcha #FarmersProtest #NoFarmersNoF...</td>\n",
       "      <td>1364506144002088963</td>\n",
       "      <td>{'username': 'KotiaPreet', 'displayname': 'Pre...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506144002088963</td>\n",
       "      <td>und</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>[{'previewUrl': 'https://pbs.twimg.com/media/E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://twitter.com/ArjunSinghPanam/status/136...   \n",
       "1  https://twitter.com/PrdeepNain/status/13645062...   \n",
       "2  https://twitter.com/parmarmaninder/status/1364...   \n",
       "3  https://twitter.com/anmoldhaliwal/status/13645...   \n",
       "4  https://twitter.com/KotiaPreet/status/13645061...   \n",
       "\n",
       "                       date  \\\n",
       "0 2021-02-24 09:23:35+00:00   \n",
       "1 2021-02-24 09:23:32+00:00   \n",
       "2 2021-02-24 09:23:22+00:00   \n",
       "3 2021-02-24 09:23:16+00:00   \n",
       "4 2021-02-24 09:23:10+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  The world progresses while the Indian police a...   \n",
       "1  #FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...   \n",
       "2  ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...   \n",
       "3  @ReallySwara @rohini_sgh watch full video here...   \n",
       "4  #KisanEktaMorcha #FarmersProtest #NoFarmersNoF...   \n",
       "\n",
       "                                     renderedContent                   id  \\\n",
       "0  The world progresses while the Indian police a...  1364506249291784198   \n",
       "1  #FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...  1364506237451313155   \n",
       "2  ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...  1364506195453767680   \n",
       "3  @ReallySwara @rohini_sgh watch full video here...  1364506167226032128   \n",
       "4  #KisanEktaMorcha #FarmersProtest #NoFarmersNoF...  1364506144002088963   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'username': 'ArjunSinghPanam', 'displayname':...   \n",
       "1  {'username': 'PrdeepNain', 'displayname': 'Pra...   \n",
       "2  {'username': 'parmarmaninder', 'displayname': ...   \n",
       "3  {'username': 'anmoldhaliwal', 'displayname': '...   \n",
       "4  {'username': 'KotiaPreet', 'displayname': 'Pre...   \n",
       "\n",
       "                                            outlinks  \\\n",
       "0  [https://twitter.com/ravisinghka/status/136415...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                     [https://youtu.be/-bUKumwq-J8]   \n",
       "4                                                 []   \n",
       "\n",
       "                 tcooutlinks  replyCount  retweetCount  likeCount  quoteCount  \\\n",
       "0  [https://t.co/es3kn0IQAF]           0             0          0           0   \n",
       "1                         []           0             0          0           0   \n",
       "2                         []           0             0          0           0   \n",
       "3  [https://t.co/wBPNdJdB0n]           0             0          0           0   \n",
       "4                         []           0             0          0           0   \n",
       "\n",
       "        conversationId lang  \\\n",
       "0  1364506249291784198   en   \n",
       "1  1364506237451313155   en   \n",
       "2  1364506195453767680   pa   \n",
       "3  1364350947099484160   en   \n",
       "4  1364506144002088963  und   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...   \n",
       "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                             sourceUrl          sourceLabel  \\\n",
       "0   http://twitter.com/download/iphone   Twitter for iPhone   \n",
       "1  http://twitter.com/download/android  Twitter for Android   \n",
       "2  http://twitter.com/download/android  Twitter for Android   \n",
       "3           https://mobile.twitter.com      Twitter Web App   \n",
       "4   http://twitter.com/download/iphone   Twitter for iPhone   \n",
       "\n",
       "                                               media  retweetedTweet  \\\n",
       "0                                               None             NaN   \n",
       "1  [{'thumbnailUrl': 'https://pbs.twimg.com/ext_t...             NaN   \n",
       "2                                               None             NaN   \n",
       "3  [{'thumbnailUrl': 'https://pbs.twimg.com/ext_t...             NaN   \n",
       "4  [{'previewUrl': 'https://pbs.twimg.com/media/E...             NaN   \n",
       "\n",
       "                                         quotedTweet  \\\n",
       "0  {'url': 'https://twitter.com/RaviSinghKA/statu...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                      mentionedUsers  \n",
       "0  [{'username': 'narendramodi', 'displayname': '...  \n",
       "1  [{'username': 'Kisanektamorcha', 'displayname'...  \n",
       "2                                               None  \n",
       "3  [{'username': 'ReallySwara', 'displayname': 'S...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(file_path, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción de las Columnas del DataFrame\n",
    "\n",
    "1. **url**: La URL del tweet.\n",
    "2. **date**: La fecha y hora en que se creó el tweet, en formato UTC.\n",
    "3. **content**: El texto del tweet.\n",
    "4. **renderedContent**: El contenido renderizado del tweet (puede ser igual a `content`).\n",
    "5. **id**: El identificador único del tweet.\n",
    "6. **user**: Informacion sobre usuario que publicó el tweet.\n",
    "7. **outlinks**: Enlaces externos incluidos en el tweet.\n",
    "8. **tcooutlinks**: Enlaces acortados de Twitter incluidos en el tweet.\n",
    "9. **replyCount**: Número de respuestas al tweet.\n",
    "10. **retweetCount**: Número de retweets del tweet.\n",
    "11. **likeCount**: Número de \"me gusta\" que recibió el tweet.\n",
    "12. **quoteCount**: Número de veces que el tweet fue citado.\n",
    "13. **conversationId**: Identificador de la conversación a la que pertenece el tweet.\n",
    "14. **lang**: El idioma del tweet, en formato de código BCP 47.\n",
    "15. **source**: La fuente desde donde se publicó el tweet (por ejemplo, \"Twitter Web Client\").\n",
    "16. **sourceUrl**: URL de la fuente desde donde se publicó el tweet.\n",
    "17. **sourceLabel**: Etiqueta de la fuente desde donde se publicó el tweet.\n",
    "18. **media**: Información sobre los medios incluidos en el tweet (imágenes, videos, etc.).\n",
    "19. **retweetedTweet**: Información sobre el tweet original si este es un retweet.\n",
    "20. **quotedTweet**: Información sobre el tweet citado.\n",
    "21. **mentionedUsers**: Usuarios mencionados en el tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de observaciones es 117407\n"
     ]
    }
   ],
   "source": [
    "# Ahora verifiquemos la cantidad de observaciones\n",
    "print('El número de observaciones es', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Las Top 10 Fechas con Más Tweets y el Usuario Más Activo en Cada Fecha</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`q1_time:` Está optimizada para el tiempo de ejecución. Utiliza pandas para cargar y procesar todo el archivo en memoria, lo que permite realizar operaciones vectorizadas rápidas y eficientes en términos de tiempo, pero puede consumir más memoria, una vez cargado el df en memoria se podria hacer todas las operaciones necesarias de froma muy rapida.\n",
    "\n",
    "`q1_memory:` Está optimizada para el uso de memoria. Utiliza técnicas que requieren menos memoria, como procesar el archivo línea por línea en lugar de cargarlo todo en memoria a la vez. Esto puede ser más lento debido a la falta de operaciones vectorizadas, pero reduce el uso de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de q1_time: 6.856872081756592 segundos\n",
      "Uso de memoria de q1_time: 1794.34375 MiB\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de las\n",
    "    10 fechas con más tweets y el nombre del usuario que más publicó en cada una de esas fechas.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[datetime.date, str]]: Una lista de tuplas donde cada tupla contiene una fecha (datetime.date)\n",
    "    y el nombre del usuario que más publicó en esa fecha.\n",
    "    \"\"\"\n",
    "    # Cargar el archivo JSON en un DataFrame de pandas\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    \n",
    "    # Convertir la columna 'date' a datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Obtener las 10 fechas con más tweets\n",
    "    top_dates = df['date'].dt.date.value_counts().nlargest(10)\n",
    "    \n",
    "    # Crear una lista de tuplas con las fechas y el nombre de usuario más activo en cada fecha\n",
    "    result = [(date, df[df['date'].dt.date == date]['user'].apply(lambda x: x['username']).value_counts().idxmax()) for date in top_dates.index]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_time = memory_usage((q1_time, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q1_time: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q1_time: {max(mem_usage_time) - min(mem_usage_time)} MiB\")\n",
    "print(q1_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de q1_memory: 2.625170946121216 segundos\n",
      "Uso de memoria de q1_memory: 1.0 MiB\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de las\n",
    "    10 fechas con más tweets y el nombre del usuario que más publicó en cada una de esas fechas,\n",
    "    optimizando para el uso de memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[datetime.date, str]]: Una lista de tuplas donde cada tupla contiene una fecha (datetime.date)\n",
    "    y el nombre del usuario que más publicó en esa fecha.\n",
    "    \"\"\"\n",
    "    # Estructura para contar los tweets por fecha y por usuario\n",
    "    date_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Procesar el archivo línea por línea para minimizar el uso de memoria\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line) # Convert to Python Dict\n",
    "            date = datetime.date.fromisoformat(tweet['date'][:10])\n",
    "            user = tweet['user']['username']\n",
    "            date_counts[date][user] += 1 # Incrementar para la fecha y usuario correspondiente\n",
    "\n",
    "    # Obtener las 10 fechas con más tweets\n",
    "    top_dates = sorted(date_counts.items(), key=lambda x: sum(x[1].values()), reverse=True)[:10] #x[1] conteos por usuario\n",
    "\n",
    "    # Crear una lista de tuplas con las fechas y el nombre de usuario más activo en cada fecha\n",
    "    result = [(date, max(users.items(), key=lambda x: x[1])[0]) for date, users in top_dates] #sum(users.values())\n",
    "\n",
    "    return result\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_memory = memory_usage((q1_memory, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q1_memory: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q1_memory: {max(mem_usage_memory) - min(mem_usage_memory)} MiB\")\n",
    "print(q1_memory(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado\n",
    "\n",
    "`q1_memory` es más eficiente tanto en términos de tiempo de ejecución como de uso de memoria para este conjunto de datos específico. Aunque pandas es generalmente muy eficiente para operaciones vectorizadas, el tiempo y la memoria necesarios para cargar un archivo JSON grande pueden no justificar su uso en todos los casos. Si el archivo JSON es pequeño o de tamaño moderado, la carga completa en un DataFrame de pandas puede ser lo suficientemente rápida y eficiente, aprovechando las operaciones vectorizadas de pandas para procesar los datos rápidamente. Si el archivo JSON está estructurado de una manera más plana y simple, pandas puede cargar y procesar los datos más rápidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Los Top 10 Emojis Más Usados con su Respectivo Conteo</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`q2_time:` Está optimizada para el tiempo de ejecución. Utiliza pandas para cargar y procesar todo el archivo en memoria, lo que permite realizar operaciones vectorizadas rápidas y eficientes en términos de tiempo, pero puede consumir más memoria, una vez cargado el df en memoria se podria hacer todas las operaciones necesarias de froma muy rapida.\n",
    "\n",
    "`q2_memory:` Está optimizada para el uso de memoria. Utiliza técnicas que requieren menos memoria, como procesar el archivo línea por línea en lugar de cargarlo todo en memoria a la vez. Esto puede ser más lento debido a la falta de operaciones vectorizadas, pero reduce el uso de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def extract_emojis_from_text(text):\n",
    "    return [c for c in text if c in emoji.EMOJI_DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de los\n",
    "    10 emojis más usados con su respectivo conteo.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[str, int]]: Una lista de tuplas donde cada tupla contiene un emoji (str) y su conteo (int).\n",
    "    \"\"\"\n",
    "    # Cargar el archivo JSON en un DataFrame de pandas\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    \n",
    "    # Extraer todos los emojis de los tweets\n",
    "    df['emojis'] = df['content'].apply(extract_emojis_from_text)\n",
    "    \n",
    "    # Contar la frecuencia de cada emoji\n",
    "    all_emojis = df['emojis'].explode() # serie de emojis\n",
    "    top_emojis = all_emojis.value_counts().nlargest(10)\n",
    "    \n",
    "    # Crear una lista de tuplas con los emojis y su conteo\n",
    "    result = list(top_emojis.items())\n",
    "    \n",
    "    #total_emojis = all_emojis.count()\n",
    "    #result = [(emoji, count, (count / total_emojis) * 100) for emoji, count in top_emojis.items()]\n",
    "    return result\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_time = memory_usage((q2_time, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q2_time: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q2_time: {max(mem_usage_time) - min(mem_usage_time)} MiB\")\n",
    "print(q2_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de los\n",
    "    10 emojis más usados con su respectivo conteo, optimizando para el uso de memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[str, int]]: Una lista de tuplas donde cada tupla contiene un emoji (str) y su conteo (int).\n",
    "    \"\"\"\n",
    "    # Estructura para contar los emojis\n",
    "    emoji_counts = defaultdict(int)\n",
    "    #total_emojis = 0\n",
    "\n",
    "    # Procesar el archivo línea por línea para minimizar el uso de memoria\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            content = tweet['content']\n",
    "            emojis_in_tweet = extract_emojis_from_text(content)\n",
    "            for em in emojis_in_tweet:\n",
    "                emoji_counts[em] += 1\n",
    "                #total_emojis += 1\n",
    "\n",
    "    # Obtener los 10 emojis más usados\n",
    "    top_emojis = sorted(emoji_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    #result = [(emoji, count, (count / total_emojis) * 100) for emoji, count in top_emojis]\n",
    "\n",
    "    return top_emojis\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_memory = memory_usage((q2_memory, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q2_memory: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q2_memory: {max(mem_usage_memory) - min(mem_usage_memory)} MiB\")\n",
    "print(q2_memory(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado\n",
    "\n",
    "`q2_memory` es más eficiente tanto en términos de tiempo de ejecución como de uso de memoria para este conjunto de datos específico. Aunque pandas es generalmente muy eficiente para operaciones vectorizadas, el tiempo y la memoria necesarios para cargar un archivo JSON grande pueden no justificar su uso en todos los casos. La elección del método de procesamiento depende del tamaño y estructura del archivo de datos y de los recursos del sistema disponibles. En situaciones donde se dispone de suficiente memoria y el archivo de datos no es excesivamente grande, el enfoque basado en pandas puede ser adecuado y eficiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Top 10 Histórico de Usuarios Más Influyentes en Función del Conteo de las Menciones (@)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`q3_time:` Está optimizada para el tiempo de ejecución. \n",
    "`q3_memory:` Está optimizada para el uso de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_mentions(s):\n",
    "    return re.findall(r'@\\w+', s)\n",
    "\n",
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de los\n",
    "    10 usuarios más mencionados con su respectivo conteo.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[str, int]]: Una lista de tuplas donde cada tupla contiene un usuario (str) y su conteo de menciones (int).\n",
    "    \"\"\"\n",
    "    # Cargar el archivo JSON en un DataFrame de pandas\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    \n",
    "    # Extraer todas las menciones de los tweets\n",
    "    df['mentions'] = df['content'].apply(extract_mentions)\n",
    "    \n",
    "    # Contar la frecuencia de cada mención\n",
    "    all_mentions = df['mentions'].explode()\n",
    "    top_mentions = all_mentions.value_counts().nlargest(10)\n",
    "    \n",
    "    # Crear una lista de tuplas con los usuarios y su conteo de menciones\n",
    "    result = list(top_mentions.items())\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_time = memory_usage((q3_time, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q3_time: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q3_time: {max(mem_usage_time) - min(mem_usage_time)} MiB\")\n",
    "print(q3_time(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de los\n",
    "    10 usuarios más mencionados con su respectivo conteo, optimizando para el uso de memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[str, int]]: Una lista de tuplas donde cada tupla contiene un usuario (str) y su conteo de menciones (int).\n",
    "    \"\"\"\n",
    "    # Estructura para contar las menciones\n",
    "    mention_counts = defaultdict(int)\n",
    "\n",
    "    # Procesar el archivo línea por línea para minimizar el uso de memoria\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            content = tweet['content']\n",
    "            mentions_in_tweet = extract_mentions(content)\n",
    "            for mention in mentions_in_tweet:\n",
    "                mention_counts[mention] += 1\n",
    "\n",
    "    # Obtener los 10 usuarios más mencionados\n",
    "    top_mentions = sorted(mention_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    return top_mentions\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_memory = memory_usage((q3_memory, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q3_memory: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q3_memory: {max(mem_usage_memory) - min(mem_usage_memory)} MiB\")\n",
    "print(q3_memory(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado\n",
    "\n",
    "`q3_memory` es más eficiente tanto en términos de tiempo de ejecución como de uso de memoria para este conjunto de datos específico. Aunque pandas es generalmente muy eficiente para operaciones vectorizadas, el tiempo y la memoria necesarios para cargar un archivo JSON grande pueden no justificar su uso en todos los casos. La elección del método de procesamiento depende del tamaño y estructura del archivo de datos y de los recursos del sistema disponibles. En situaciones donde se dispone de suficiente memoria y el archivo de datos no es excesivamente grande, el enfoque basado en pandas puede ser adecuado y eficiente. Sin embargo, para archivos de gran tamaño o sistemas con limitaciones de memoria, procesar el archivo línea por línea es una estrategia más robusta y eficiente. Evaluar estas variables es crucial para seleccionar la estrategia de procesamiento más adecuada para cada caso particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque Distribuido en la Nube *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Las Top 10 Fechas con Más Tweets y el Usuario Más Activo en Cada Fecha</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, to_date,split, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import datetime\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Twitter Mentions Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "def q1_time_spark(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de las\n",
    "    10 fechas con más tweets y el nombre del usuario que más publicó en cada una de esas fechas.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[datetime.date, str]]: Una lista de tuplas donde cada tupla contiene una fecha (datetime.date)\n",
    "    y el nombre del usuario que más publicó en esa fecha.\n",
    "    \"\"\"\n",
    "    # Cargar el archivo JSON en un DataFrame de Spark\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    # Convertir la columna 'date' a date\n",
    "    df = df.withColumn('date', to_date(col('date')))\n",
    "    \n",
    "    # Obtener las 10 fechas con más tweets\n",
    "    top_dates = df.groupBy('date').count().orderBy('count', ascending=False).limit(10).collect()\n",
    "    \n",
    "    # Crear una lista de tuplas con las fechas y el nombre de usuario más activo en cada fecha\n",
    "    result = []\n",
    "    for row in top_dates:\n",
    "        date = row['date']\n",
    "        top_user = df.filter(col('date') == date).groupBy('user.username').count().orderBy('count', ascending=False).first()['username']\n",
    "        result.append((date, top_user))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_time = memory_usage((q1_time_spark, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q1_time_spark: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q1_time_spark: {max(mem_usage_time) - min(mem_usage_time)} MiB\")\n",
    "print(q1_time_spark(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Los Top 10 Emojis Más Usados con su Respectivo Conteo</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "def extract_emojis_from_text(text):\n",
    "    return ''.join([c for c in text if c in emoji.EMOJI_DATA])\n",
    "\n",
    "def q2_time_spark(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Esta función toma la ruta de un archivo JSON que contiene tweets y devuelve una lista de los\n",
    "    10 emojis más usados con su respectivo conteo.\n",
    "\n",
    "    Parámetros:\n",
    "    file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
    "\n",
    "    Retorna:\n",
    "    List[Tuple[str, int]]: Una lista de tuplas donde cada tupla contiene un emoji (str) y su conteo (int).\n",
    "    \"\"\"\n",
    "    # Cargar el archivo JSON en un DataFrame de Spark\n",
    "    df = spark.read.json(file_path)\n",
    "    \n",
    "    # Extraer todos los emojis de los tweets\n",
    "    extract_emojis_udf = spark.udf.register(\"extract_emojis_udf\", extract_emojis_from_text)\n",
    "    df = df.withColumn('emojis', extract_emojis_udf(col('content')))\n",
    "    \n",
    "    # Explode la columna de emojis\n",
    "    df = df.withColumn('emoji', explode(split(col('emojis'), '')))\n",
    "    \n",
    "    # Contar la frecuencia de cada emoji\n",
    "    emoji_counts = df.groupBy('emoji').count().orderBy('count', ascending=False).limit(10)\n",
    "    \n",
    "    # Crear una lista de tuplas con los emojis y su conteo\n",
    "    result = [(row['emoji'], row['count']) for row in emoji_counts.collect()]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_time = memory_usage((q2_time_spark, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q2_time_spark: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q2_time_spark: {max(mem_usage_time) - min(mem_usage_time)} MiB\")\n",
    "print(q2_time_spark(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Top 10 Histórico de Usuarios Más Influyentes en Función del Conteo de las Menciones (@)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# función extract_mentions\n",
    "def extract_mentions(s):\n",
    "    return re.findall(r'@\\w+', s)\n",
    "\n",
    "# extract_mentions como UDF\n",
    "extract_mentions_udf = udf(extract_mentions, ArrayType(StringType()))\n",
    "\n",
    "def q3_time_spark(file_path: str) -> List[Tuple[str, int]]:\n",
    "    df = spark.read.json(file_path)\n",
    "    df = df.withColumn('mentions', explode(extract_mentions_udf(col('content'))))\n",
    "    mention_counts = df.groupBy('mentions').count().orderBy('count', ascending=False).limit(10)\n",
    "    result = [(row['mentions'], row['count']) for row in mention_counts.collect()]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Medir tiempo de ejecución\n",
    "start_time = time.time()\n",
    "mem_usage_time = memory_usage((q3_time_spark, (file_path,)), interval=0.1, timeout=None)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Tiempo de ejecución de q3_time_spark: {end_time - start_time} segundos\")\n",
    "print(f\"Uso de memoria de q3_time_spark: {max(mem_usage_time) - min(mem_usage_time)} MiB\")\n",
    "print(q3_time_spark(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de Usar un Enfoque Distribuido\n",
    "\n",
    "El uso de un enfoque distribuido para el procesamiento de datos tiene varias ventajas significativas, especialmente cuando se trabaja con grandes volúmenes de datos. Plataformas como Databricks, AWS (con AWS Glue), Azure (con Azure Synapse Analytics), y GCP (con Google BigQuery) ofrecen servicios robustos que facilitan la implementación y gestión de clústeres de computación distribuida.\n",
    "\n",
    "1. **Escalabilidad**: Un enfoque distribuido permite escalar horizontalmente añadiendo más nodos al clúster. Esto significa que a medida que el volumen de datos crece, se pueden agregar más recursos para manejar la carga adicional, asegurando un rendimiento consistente.\n",
    "\n",
    "2. **Eficiencia de Procesamiento**: Tecnologías como Apache Spark permiten el procesamiento paralelo de datos. Esto reduce significativamente el tiempo de ejecución para tareas que involucran grandes conjuntos de datos, ya que múltiples nodos pueden trabajar simultáneamente en diferentes partes del conjunto de datos.\n",
    "\n",
    "3. **Tolerancia a Fallos**: Los sistemas distribuidos están diseñados para ser resilientes a fallos. Si un nodo falla, las tareas se pueden reasignar a otros nodos, asegurando que el procesamiento continúe sin interrupciones significativas.\n",
    "\n",
    "4. **Flexibilidad y Facilidad de Uso**: Plataformas como Databricks proporcionan entornos integrados que facilitan la configuración y el uso de clústeres distribuidos. Además, ofrecen integraciones con una variedad de servicios en la nube, almacenamiento de datos, y herramientas de análisis, proporcionando un entorno de trabajo flexible y eficiente.\n",
    "\n",
    "Sin embargo, es importante considerar que para conjuntos de datos pequeños, como el ejemplo de 398 MB utilizado en este ejercicio, las ventajas de un enfoque distribuido pueden no ser tan evidentes. La sobrecarga asociada con la configuración y gestión de clústeres puede superar los beneficios del procesamiento paralelo, resultando en tiempos de ejecución comparables o incluso mayores que los de un procesamiento local. No obstante, un enfoque distribuido se vuelve altamente beneficioso y necesario cuando se espera un crecimiento significativo en el volumen de datos o se requiere un procesamiento robusto y eficiente en un entorno de producción.\n",
    "\n",
    "### Consideraciones de Escalabilidad\n",
    "\n",
    "Para conjuntos de datos más grandes, el uso de Spark en un clúster distribuido proporciona una solución escalable y eficiente. A medida que el tamaño de los datos crece, el enfoque distribuido permite manejar la carga adicional sin degradar el rendimiento, algo que sería difícil de lograr con un enfoque local. Por lo tanto, para aplicaciones que implican grandes volúmenes de datos y requieren un rendimiento consistente, plataformas distribuidas como Databricks, AWS Glue, Azure Synapse Analytics, y Google BigQuery son las mejores opciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
